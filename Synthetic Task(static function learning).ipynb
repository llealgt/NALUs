{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiments based on section 4.1  of arXiv:1808.00508 paper: Simple Function Learning Tasks(static task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from NALU import NALU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_shape = (10000,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_examples = x_shape[0]\n",
    "train_columns  = x_shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.random.rand(train_examples,train_columns)*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_x = np.random.rand(train_examples,train_columns)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition the inputs(consistently) to create \"a\" and \"b\"\n",
    "* The paper does not describe if \"a\" and \"b\" are mutually exclusive or not, will have to experiment\n",
    "* In the paper its not clear if for every experiment you  mix (+,-,*,/)   for calculating \"y\", let's experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mutually exclusive(first half is a, second half is b)  and only use \"+\" for calculating \"y\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "partition_index = int(train_columns/2) #half colmns for a,half columns for b\n",
    "\n",
    "a = np.sum(x[:,:partition_index],axis=1)\n",
    "b = np.sum( x[:,partition_index:],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_a = np.sum(test_x[:,:partition_index],axis=1)\n",
    "test_b = np.sum(test_x[:,partition_index:],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's start simple:  y = a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = a + b\n",
    "y = np.expand_dims(y,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_y = test_a + test_b\n",
    "test_y = np.expand_dims(test_y,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 50000\n",
    "PRINT_EVERY = 1000\n",
    "LEARNING_RATE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello NALU world nalu1\n",
      "hello NALU world nalu2\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32,shape = [train_examples,train_columns])\n",
    "Y = tf.placeholder(tf.float32,shape=[train_examples,1])\n",
    "\n",
    "nalu1 = NALU(input_shape=(train_examples,train_columns),size = 2,name = \"nalu1\")\n",
    "nalu1_output = nalu1.NALU_output(X)\n",
    "        \n",
    "nalu2 = NALU(input_shape=(train_examples,2),size=1,name = \"nalu2\")\n",
    "nalu2_output = nalu2.NALU_output(nalu1_output)\n",
    "        \n",
    "loss = tf.losses.mean_squared_error(nalu2_output,Y)\n",
    "adam_optimize = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss 7546862.0\n",
      "Epoch 10000 loss 80.97679138183594\n",
      "Epoch 20000 loss 0.5189469456672668\n",
      "Epoch 30000 loss 0.0034753356594592333\n",
      "Epoch 40000 loss 2.3557857275591232e-05\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.Session() as session:\n",
    "        \n",
    "    session.run(tf.global_variables_initializer())\n",
    "        \n",
    "    for epoch in range(EPOCHS):\n",
    "        _  = session.run(adam_optimize,feed_dict={X:x,Y:y})\n",
    "            \n",
    "        if epoch % PRINT_EVERY == 0:\n",
    "            batch_loss = session.run(loss,feed_dict={X:x,Y:y})\n",
    "            print(\"Epoch {} loss {}\".format(epoch,batch_loss))\n",
    "                \n",
    "    \n",
    "    test_predictions,test_loss = session.run([nalu2_output,loss],feed_dict={X:test_x,Y:test_y})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5262.3984375 ],\n",
       "       [ 4994.34765625],\n",
       "       [ 5116.49414062],\n",
       "       [ 4841.08203125],\n",
       "       [ 4826.49023438],\n",
       "       [ 5159.26806641],\n",
       "       [ 4616.74511719],\n",
       "       [ 4961.86474609],\n",
       "       [ 4885.41113281],\n",
       "       [ 5435.7265625 ],\n",
       "       [ 4920.23779297],\n",
       "       [ 4855.30957031],\n",
       "       [ 4799.19775391],\n",
       "       [ 5126.62060547],\n",
       "       [ 5253.20117188],\n",
       "       [ 5191.59277344],\n",
       "       [ 5140.27929688],\n",
       "       [ 4683.34863281],\n",
       "       [ 4890.54931641],\n",
       "       [ 5032.58789062],\n",
       "       [ 4843.16894531],\n",
       "       [ 5310.77929688],\n",
       "       [ 4785.85107422],\n",
       "       [ 4847.19628906],\n",
       "       [ 5733.85058594],\n",
       "       [ 4655.84082031],\n",
       "       [ 5012.56054688],\n",
       "       [ 4838.69042969],\n",
       "       [ 5077.61621094],\n",
       "       [ 5308.90185547]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5262.40268932],\n",
       "       [ 4994.35150524],\n",
       "       [ 5116.49861588],\n",
       "       [ 4841.0859763 ],\n",
       "       [ 4826.49370747],\n",
       "       [ 5159.27207757],\n",
       "       [ 4616.74875958],\n",
       "       [ 4961.86871301],\n",
       "       [ 4885.41553587],\n",
       "       [ 5435.73062546],\n",
       "       [ 4920.24171355],\n",
       "       [ 4855.31355142],\n",
       "       [ 4799.20156366],\n",
       "       [ 5126.62456043],\n",
       "       [ 5253.20560224],\n",
       "       [ 5191.59725647],\n",
       "       [ 5140.28352116],\n",
       "       [ 4683.35173228],\n",
       "       [ 4890.55327443],\n",
       "       [ 5032.59177821],\n",
       "       [ 4843.17296524],\n",
       "       [ 5310.7837354 ],\n",
       "       [ 4785.85483149],\n",
       "       [ 4847.19992523],\n",
       "       [ 5733.85493744],\n",
       "       [ 4655.84420023],\n",
       "       [ 5012.56422833],\n",
       "       [ 4838.69378524],\n",
       "       [ 5077.62022844],\n",
       "       [ 5308.90643617]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5600663e-05"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Almost 0 test error!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Let's test substraction:  y = a - b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 50000\n",
    "PRINT_EVERY = 1000\n",
    "LEARNING_RATE = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = a - b\n",
    "y = np.expand_dims(y,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_y = test_a - test_b\n",
    "test_y = np.expand_dims(test_y,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello NALU world nalu1\n",
      "hello NALU world nalu2\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32,shape = [train_examples,train_columns])\n",
    "Y = tf.placeholder(tf.float32,shape=[train_examples,1])\n",
    "\n",
    "nalu1 = NALU(input_shape=(train_examples,train_columns),size = 2,name = \"nalu1\")\n",
    "nalu1_output = nalu1.NALU_output(X)\n",
    "        \n",
    "nalu2 = NALU(input_shape=(train_examples,2),size=1,name = \"nalu2\")\n",
    "nalu2_output = nalu2.NALU_output(nalu1_output)\n",
    "        \n",
    "loss = tf.losses.mean_squared_error(nalu2_output,Y)\n",
    "adam_optimize = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss 846.255615234375\n",
      "Epoch 1000 loss 14.203326225280762\n",
      "Epoch 2000 loss 60.006011962890625\n",
      "Epoch 3000 loss 5.976831912994385\n",
      "Epoch 4000 loss 3.5120203495025635\n",
      "Epoch 5000 loss 2.2812371253967285\n",
      "Epoch 6000 loss 1.5396976470947266\n",
      "Epoch 7000 loss 1.3886674642562866\n",
      "Epoch 8000 loss 0.5454602241516113\n",
      "Epoch 9000 loss 0.0023647500202059746\n",
      "Epoch 10000 loss 0.0005154729005880654\n",
      "Epoch 11000 loss 0.000221734240767546\n",
      "Epoch 12000 loss 0.0001145371570601128\n",
      "Epoch 13000 loss 0.2683952748775482\n",
      "Epoch 14000 loss 5.105063974042423e-05\n",
      "Epoch 15000 loss 2.709268665057607e-05\n",
      "Epoch 16000 loss 0.00014868692960590124\n",
      "Epoch 17000 loss 7.774979167152196e-06\n",
      "Epoch 18000 loss 4.583665941026993e-06\n",
      "Epoch 19000 loss 3.503787866065977e-06\n",
      "Epoch 20000 loss 0.007569760084152222\n",
      "Epoch 21000 loss 3.8780794966442045e-06\n",
      "Epoch 22000 loss 1.2697902320724097e-06\n",
      "Epoch 23000 loss 3.4290827898075804e-05\n",
      "Epoch 24000 loss 0.4637410044670105\n",
      "Epoch 25000 loss 4.006538802059367e-05\n",
      "Epoch 26000 loss 7.389058964690776e-08\n",
      "Epoch 27000 loss 0.0095549076795578\n",
      "Epoch 28000 loss 1.5401615982568728e-08\n",
      "Epoch 29000 loss 0.0012202406069263816\n",
      "Epoch 30000 loss 4.58676295238547e-05\n",
      "Epoch 31000 loss 5.425845550632857e-09\n",
      "Epoch 32000 loss 2.055825065738759e-09\n",
      "Epoch 33000 loss 1.938283458002843e-05\n",
      "Epoch 34000 loss 2.5497476130453833e-08\n",
      "Epoch 35000 loss 6.239646022088152e-10\n",
      "Epoch 36000 loss 5.474121156368028e-10\n",
      "Epoch 37000 loss 5.706903838387234e-09\n",
      "Epoch 38000 loss 3.154362326895921e-09\n",
      "Epoch 39000 loss 0.0010243664728477597\n",
      "Epoch 40000 loss 5.519218415628302e-09\n",
      "Epoch 41000 loss 6.437103738932137e-09\n",
      "Epoch 42000 loss 1.5961731936187107e-10\n",
      "Epoch 43000 loss 1.505135855950357e-07\n",
      "Epoch 44000 loss 2.836360692981543e-07\n",
      "Epoch 45000 loss 9.870282369206507e-10\n",
      "Epoch 46000 loss 4.895935944659868e-06\n",
      "Epoch 47000 loss 0.0014006330166012049\n",
      "Epoch 48000 loss 3.732838013092987e-05\n",
      "Epoch 49000 loss 1.7094816229246135e-08\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.Session() as session:\n",
    "        \n",
    "    session.run(tf.global_variables_initializer())\n",
    "        \n",
    "    for epoch in range(EPOCHS):\n",
    "        _  = session.run(adam_optimize,feed_dict={X:x,Y:y})\n",
    "            \n",
    "        if epoch % PRINT_EVERY == 0:\n",
    "            batch_loss = session.run(loss,feed_dict={X:x,Y:y})\n",
    "            print(\"Epoch {} loss {}\".format(epoch,batch_loss))\n",
    "                \n",
    "    \n",
    "    test_predictions,test_loss = session.run([nalu2_output,loss],feed_dict={X:test_x,Y:test_y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-439.46615601],\n",
       "       [ -49.69976807],\n",
       "       [ 614.57897949],\n",
       "       [  96.49380493],\n",
       "       [ -28.91046143],\n",
       "       [ -36.49261475],\n",
       "       [ 554.80273438],\n",
       "       [-334.08990479],\n",
       "       [-259.23608398],\n",
       "       [-145.85144043],\n",
       "       [ 433.85076904],\n",
       "       [   4.83081055],\n",
       "       [ -15.04400635],\n",
       "       [-122.73522949],\n",
       "       [  40.76782227],\n",
       "       [-322.97564697],\n",
       "       [-386.14755249],\n",
       "       [-255.42050171],\n",
       "       [-381.76763916],\n",
       "       [ 429.49520874],\n",
       "       [ 765.99078369],\n",
       "       [ 128.36804199],\n",
       "       [-152.26296997],\n",
       "       [  11.20697021],\n",
       "       [-388.90026855],\n",
       "       [ 345.62643433],\n",
       "       [-311.68539429],\n",
       "       [-319.4163208 ],\n",
       "       [ 176.41564941],\n",
       "       [  87.73461914]], dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-439.4672283 ],\n",
       "       [ -49.70085501],\n",
       "       [ 614.5779377 ],\n",
       "       [  96.49282442],\n",
       "       [ -28.91142763],\n",
       "       [ -36.4935651 ],\n",
       "       [ 554.80180936],\n",
       "       [-334.09095911],\n",
       "       [-259.23711938],\n",
       "       [-145.85252448],\n",
       "       [ 433.84976148],\n",
       "       [   4.82981916],\n",
       "       [ -15.04500564],\n",
       "       [-122.73620444],\n",
       "       [  40.76666537],\n",
       "       [-322.97665024],\n",
       "       [-386.14852699],\n",
       "       [-255.42146743],\n",
       "       [-381.76862416],\n",
       "       [ 429.49431609],\n",
       "       [ 765.98980743],\n",
       "       [ 128.36692693],\n",
       "       [-152.26386003],\n",
       "       [  11.20606413],\n",
       "       [-388.90144714],\n",
       "       [ 345.62557668],\n",
       "       [-311.68637517],\n",
       "       [-319.41728345],\n",
       "       [ 176.41457705],\n",
       "       [  87.73359063]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0464798e-06"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Again almost 0 test error!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conclusions/comments\n",
    "* NALU is very sensitive to parameters initialization, xavier initialization doesnt seemed to help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-439.4672283 ],\n",
       "       [ -49.70085501],\n",
       "       [ 614.5779377 ],\n",
       "       ..., \n",
       "       [ 173.65368772],\n",
       "       [ -28.21450596],\n",
       "       [ 182.63540218]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
